---
title: "CODE"
author: "Chun-Li Hou"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    number_sections: true
    theme: united
    highlight: tango
---

```{r setup, include = F}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center",
                      message = FALSE, warning = FALSE)
```

```{r, echo = F}
if(!require("pacman")) install.packages("pacman")
pacman::p_load(dplyr, rvest, readr, stringr, utf8, tidyr, httr, purrr,
               pageviews)
```

# Objective

This project includes different parts of the case study. The following case studies are from mainly Google search and other learning platform courses, such as Datacamp, Google, Rpubs, and Coursera. The center point of these case studies is web scraping, which covers from basic scraping, auto crawling package selenium, article analysis, word operating, and etc.

# Case Study

## A

```{r}
# source
genre.xpath = "//div[@class='subtext']/a"
movie.url = "https://www.imdb.com/title/tt4154756"

# check
status_code(GET(movie.url))

# scrap
temp = movie.url %>%
  read_html() %>%
  html_nodes(xpath = genre.xpath) %>% 
  html_text() %>% 
  gsub(pattern = "\n", replacement = "") %>%
  trimws(which = "both")

# dice
genre = temp[1:3]
date = temp[4]

# print
paste0("Avengers: Infinity War is a moive of ",
       genre[1],
       ", ",
       genre[2],
       ", and ",
       genre[3],
       ", and it showed on ",
       date)
```

## B

```{r}
mountain_wiki_pages = c("https://en.wikipedia.org/w/index.php?title=Mount_Everest&oldid=958643874",
                        "https://en.wikipedia.org/w/index.php?title=K2&oldid=956671989",
                        "https://en.wikipedia.org/w/index.php?title=Kangchenjunga&oldid=957008408")

read_html_delayed = slowly(read_html, rate = rate_delay(0.5))

for(page_url in mountain_wiki_pages){
  html = read_html_delayed(page_url)
  peak = html %>% html_node("#firstHeading") %>% html_text()
  coords = html %>% html_node("#coordinates .geo-dms") %>% html_text()
  print(paste(peak, coords, sep = ": "))}
```

## C

```{r}
# encode ch
Sys.setlocale(category = "LC_ALL", locale = "cht")

# source
id.xpath = "//div[@class='SbNwzf eeoZZ']"
id.url = "https://news.google.com/topstories?hl=zh-TW&gl=TW&ceid=TW:zh-Hant"

# scrap
temp = id.url %>% 
  read_html() %>% 
  html_nodes(xpath = id.xpath) %>% 
  html_text() %>% 
  gsub(pattern = "\n", replacement = "") %>% 
  trimws(which = "both")

# split
df = data.frame(Content = temp)
pattern = "bookmark_bordersharemore_vert"
df = df %>% 
  mutate(Content = str_split(Content, pattern = pattern)) %>% 
  unnest %>% 
  subset(Content != "")

# output
# write.csv(df, "TEST.csv", row.names = F)

# encode en
Sys.setlocale(category = "LC_ALL", locale = "English")
# Sys.getlocale()
```

## D

```{r}

```

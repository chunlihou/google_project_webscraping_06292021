---
title: "Web Scraping"
author: "Chun-Li Hou"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    theme: united
    highlight: tango
---

```{r setup, include = F}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center",
                      message = FALSE, warning = FALSE,
                      echo = F)
```

```{r}
if(!require("pacman")) install.packages("pacman")
pacman::p_load(dplyr, rvest, readr, stringr, utf8, tidyr, httr, purrr,
               pageviews, xml2, jiebaR, tidytext, igraph, topicmodels,
               ggpubr, reshape2, wordcloud2, htmlwidgets, webshot,
               kableExtra)
```

# Objective

This project includes different parts of the case study. The following case studies are from mainly Google search and other learning platform courses, such as Datacamp, Google, Rpubs, and Coursera. The center point of these case studies is web scraping, which covers from basic scraping, auto crawling package selenium, article analysis, word operating, and etc.

# Case Study

## Google News with Wordcloud

- By scraping the Google news website in Taiwan, we could have the update and breaking news on time

```{r, results = "hide"}
# encode ch
Sys.setlocale(category = "LC_ALL", locale = "cht")

# source
id.xpath = "//div[@class='SbNwzf eeoZZ']"
id.url = "https://news.google.com/topstories?hl=zh-TW&gl=TW&ceid=TW:zh-Hant"

# scrap
temp = id.url %>% 
  read_html() %>% 
  html_nodes(xpath = id.xpath) %>% 
  html_text() %>% 
  gsub(pattern = "\n", replacement = "") %>% 
  trimws(which = "both")

# process
df = data.frame(Content = temp)
pattern = "bookmark_bordersharemore_vert"

df = df %>% 
  mutate(Content = str_split(Content, pattern = pattern)) %>% 
  unnest %>% 
  subset(Content != "") %>% 
  mutate(Content = str_remove_all(Content, "[0-9a-zA-Z]+?"))

temp = paste(apply(df, 1, function(row) paste(dQuote(row), collapse = " ")),
             collapse = " ")

cutter = worker(stop_word = "STOPWORD.csv", bylines = F)
word = cutter[temp]

# analyze
df.freq = freq(word)
df.freq = arrange(df.freq, desc(freq))
df.freq.filter = df.freq %>% filter(freq > 2 & nchar(char) > 1)

# plot
p = wordcloud2(df.freq.filter,
               minSize =  2, 
               fontFamily = "Microsoft YaHei",
               size = 1)

# output
saveWidget(p, "p.html", selfcontained = F)
webshot("p.html", "p.png", vwidth = 900, vheight = 700, delay = 5)

# encode en
Sys.setlocale(category = "LC_ALL", locale = "English")
# Sys.getlocale()
```

## Wiki Celebrity in Table

- By setting celebrity's name, we can scrap down the information from Wikipedia website in English

```{r}
rm(wiki.table)
url = "https://en.wikipedia.org/w/api.php"
title = "Hadley Wickham"
# title = "Ross Ihaka"
# title = "Grace Hopper"
query = list(action = "parse", page = title, format = "xml")
resp = GET(url = url, query = query)
# status_code((resp))
resp.xml = content(resp)
page.html = resp.xml %>% xml_text() %>% read_html()
wiki.table = page.html %>% 
  html_node(css = ".infobox") %>% 
  html_table()
colnames(wiki.table) = c("key", "value")
name.df = data.frame(key = "Full name", value = title)
wiki.table = rbind(name.df, wiki.table)
wiki.table %>%
  filter(key != "") %>%
  kbl(align = "l") %>% 
  kable_styling(bootstrap_options = c("hover", "bordered", "striped"))
```
